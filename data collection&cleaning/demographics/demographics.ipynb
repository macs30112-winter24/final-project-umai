{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def process_community_info(url, city): \n",
    "    \"\"\"Processes community information from a list of strings.\n",
    "\n",
    "    Args:\n",
    "        community_info: A list of strings containing community information.\n",
    "        city: The name of the city to use for replacement.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each inner list represents grouped information for a community.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    community_info = [row.text for row in soup.findAll('div', attrs={'class': 'hgraph'})]\n",
    "\n",
    "    result = []\n",
    "    current_neighborhood = None\n",
    "    current_list = None\n",
    "\n",
    "    for item in community_info:\n",
    "        match = re.match(r'^([^:]+):(.*)', item)\n",
    "        if match:\n",
    "            name = match.group(1)\n",
    "            content = match.group(2)\n",
    "\n",
    "            if name in ['Males', 'This neighborhood']:\n",
    "                name = current_neighborhood\n",
    "            else:\n",
    "                current_neighborhood = name\n",
    "\n",
    "            content = re.sub(r'(city|{}):.*'.format(city), '', content)\n",
    "\n",
    "            if current_list and name == current_list[0]:\n",
    "                current_list.append(content)\n",
    "            else:\n",
    "                if current_list:\n",
    "                    result.append(current_list)\n",
    "                current_list = [name, content]\n",
    "\n",
    "    if current_list:\n",
    "        result.append(current_list)\n",
    "\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get population density & median age\n",
    "def create_neighborhood_dicts(result, city):\n",
    "    community_dicts = []\n",
    "    for neighborhood_data in result:\n",
    "        info = {}\n",
    "        info['community'] = neighborhood_data[0]\n",
    "\n",
    "        for item in neighborhood_data[1:]:\n",
    "            if 'people' in item:\n",
    "                match = re.findall(r'\\d+,\\d+', item)\n",
    "                if match:\n",
    "                    info['population.density'] = match[0].replace(',', '')  \n",
    "                else:\n",
    "                    info['population.density'] = None \n",
    "            if 'yearsFemales:' in item: \n",
    "                ages = re.findall(r'(\\d+\\.\\d+)', item)\n",
    "                if ages:  \n",
    "                    info['median.age.male'] = float(ages[0])  # Convert to float for calculation\n",
    "                    info['median.age.female'] = float(ages[1])\n",
    "\n",
    "        # Calculate median age and remove old keys\n",
    "        if 'median.age.male' in info and 'median.age.female' in info:\n",
    "            mean_age = info['median.age'] = (info['median.age.male'] + info['median.age.female']) / 2\n",
    "            info['median.age'] = \"{:.1f}\".format(mean_age)\n",
    "            del info['median.age.male']\n",
    "            del info['median.age.female']\n",
    "        \n",
    "        info['city'] = city\n",
    "        \n",
    "        community_dicts.append(info)\n",
    "    return community_dicts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chicago Population density & Median age\n",
    "url = 'https://www.city-data.com/nbmaps/neigh-Chicago-Illinois.html'\n",
    "result = process_community_info(url, 'Chicago')\n",
    "Chi_neighborhood_dicts = create_neighborhood_dicts(result, 'Chicago')\n",
    "df = pd.DataFrame(Chi_neighborhood_dicts)\n",
    "df.to_csv('chi_density.age.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los Angelest City Population density & Median age\n",
    "url = 'https://www.city-data.com/nbmaps/neigh-Los-Angeles-California.html'\n",
    "result = process_community_info(url, 'Los Angeles')\n",
    "LA_neighborhood_dicts = create_neighborhood_dicts(result, 'Los Angeles City')\n",
    "df = pd.DataFrame(LA_neighborhood_dicts)\n",
    "df.to_csv('la_density.age.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New York City Population density & Median age\n",
    "url = 'https://www.city-data.com/nbmaps/neigh-New-York-New-York.html'\n",
    "result = process_community_info(url, 'New York')\n",
    "NYC_neighborhood_dicts = create_neighborhood_dicts(result, 'New York City')\n",
    "df = pd.DataFrame(NYC_neighborhood_dicts)\n",
    "df.to_csv('nyc_density.age.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Population density & Median age\n",
    "chi_df = pd.DataFrame(Chi_neighborhood_dicts) \n",
    "la_df = pd.DataFrame(LA_neighborhood_dicts)\n",
    "nyc_df = pd.DataFrame(NYC_neighborhood_dicts)\n",
    "all_demo1 = pd.concat([chi_df, la_df, nyc_df], axis=0)\n",
    "all_demo1.to_csv('all_density.age.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dominant race\n",
    "def get_neighborhood_url(base_url, city_url):\n",
    "    response = requests.get(city_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    links = []\n",
    "    for element in soup.find_all(\"div\", class_=\"neighborhood\"):\n",
    "        for link in element.find_all('a', href=lambda href: href and \"neighborhood\" in href): \n",
    "            links.append(link['href'])\n",
    "    full_urls = [(base_url + link) for link in links]\n",
    "    return full_urls\n",
    "\n",
    "def get_neighborhood_souptxt(full_urls):\n",
    "    requested_pages = []\n",
    "    for index, full_url in enumerate(full_urls):            \n",
    "        requested_url = requests.get(full_url)                              \n",
    "        requested_url_txt = requested_url.text                   \n",
    "        time.sleep(random.randint(1, 2))                         \n",
    "        requested_pages.append(requested_url_txt)\n",
    "    souped_pages = []\n",
    "    for requested_page in requested_pages:    \n",
    "        souped_page = BeautifulSoup(requested_page, 'html.parser')   \n",
    "        souped_pages.append(souped_page)\n",
    "    return souped_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighborhood_domininant_race(souped_pages):\n",
    "    dominant_race = []\n",
    "    for soup in souped_pages:\n",
    "        race_info = soup.find('span', class_='badge alert-info')\n",
    "        if race_info:\n",
    "            dmn_race = race_info.find_next('b').get_text(strip=True)\n",
    "        else:\n",
    "            dmn_race = None \n",
    "        dominant_race.append(dmn_race)\n",
    "    return(dominant_race)\n",
    "\n",
    "def get_neighborhood_name(full_urls):\n",
    "    neighborhood_name = []\n",
    "    pattern = re.compile(r'-(Los-Angeles-CA|New-York-NY|Chicago-IL)\\.html$')\n",
    "    for url in full_urls:\n",
    "        name = pattern.sub('', url.split('/')[-1])\n",
    "        name = name.replace('-', ' ')\n",
    "        neighborhood_name.append(name)\n",
    "    return neighborhood_name\n",
    "\n",
    "def extract_city_name_from_url(city_url):\n",
    "    pattern = re.compile(r'neigh-(.*?)-(New-York|California|Illinois)\\.html')\n",
    "    \n",
    "    match = pattern.search(city_url)\n",
    "    if match:\n",
    "        city_name_with_dashes = match.group(1)\n",
    "        city_name = city_name_with_dashes.replace('-', ' ')\n",
    "    return city_name\n",
    "\n",
    "def get_dominant_csv(city_name, neighborhood_name, dominant_race):\n",
    "    df = pd.DataFrame(list(zip(neighborhood_name, dominant_race)), \n",
    "                      columns=['Neighborhood Name', 'Dominant Race'])\n",
    "    filename = f'C:/Users/13945/Desktop/data mgmt/umai/neighborhood_dominant_race_{city_name}.csv'\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine dominant race in three cities\n",
    "def overall(city_url, base_url):\n",
    "    full_urls = get_neighborhood_url(base_url, city_url)\n",
    "    souped_pages = get_neighborhood_souptxt(full_urls)\n",
    "    dominant_race = get_neighborhood_domininant_race(souped_pages)\n",
    "    neighborhood_name = get_neighborhood_name(full_urls)\n",
    "    city_name = extract_city_name_from_url(city_url)\n",
    "    get_dominant_csv(city_name, neighborhood_name, dominant_race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All dominant race\n",
    "chi_race = pd.read_csv('chi_race.csv')\n",
    "la_race = pd.read_csv('la_race.csv')\n",
    "nyc_race = pd.read_csv('nyc_race.csv')\n",
    "chi_df = pd.DataFrame(chi_race)\n",
    "la_df = pd.DataFrame(la_race)\n",
    "nyc_df = pd.DataFrame(nyc_race)\n",
    "all_race = pd.concat([chi_df, la_df, nyc_df], axis=0)\n",
    "all_race.to_csv('all_race.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All demographic attributes\n",
    "all_density_and_age = pd.read_csv('all_density.age.csv')\n",
    "all_race.rename(columns={'Neighborhood Name': 'community'}, inplace=True)\n",
    "all_demo = pd.merge(all_density_and_age, all_race, on=['community', 'city'])\n",
    "all_demo.to_csv('all_demo.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
